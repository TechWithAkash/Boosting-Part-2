{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f3d3d86",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient Boosting Regression is an ensemble learning technique used for regression tasks. It involves sequentially adding weak learners (usually decision trees) to an ensemble, where each new tree corrects errors made by the previous ones. It minimizes the loss function by using gradient descent optimization, fitting each new tree to the residual errors of the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc369163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4249b230",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51cc948b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 55.63481013612932\n",
      "R-squared: 0.9622905041637521\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a small dataset for regression\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=5, random_state=42)\n",
    "\n",
    "# Define the base learner (Decision Tree Regressor)\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.prediction = np.mean(y)\n",
    "\n",
    "        if self.max_depth > 0:\n",
    "            best_sse = np.inf\n",
    "\n",
    "            for feature in range(X.shape[1]):\n",
    "                feature_values = np.unique(X[:, feature])\n",
    "                for value in feature_values:\n",
    "                    left_indices = X[:, feature] <= value\n",
    "                    right_indices = X[:, feature] > value\n",
    "\n",
    "                    y_left = y[left_indices]\n",
    "                    y_right = y[right_indices]\n",
    "\n",
    "                    if len(y_left) == 0 or len(y_right) == 0:\n",
    "                        continue\n",
    "\n",
    "                    sse = np.sum((y_left - np.mean(y_left))**2) + np.sum((y_right - np.mean(y_right))**2)\n",
    "                    if sse < best_sse:\n",
    "                        best_sse = sse\n",
    "                        self.split_feature = feature\n",
    "                        self.split_value = value\n",
    "\n",
    "            if self.split_feature is not None:\n",
    "                left_indices = X[:, self.split_feature] <= self.split_value\n",
    "                right_indices = X[:, self.split_feature] > self.split_value\n",
    "\n",
    "                self.left = DecisionTree(max_depth=self.max_depth - 1)\n",
    "                self.left.fit(X[left_indices], y[left_indices])\n",
    "\n",
    "                self.right = DecisionTree(max_depth=self.max_depth - 1)\n",
    "                self.right.fit(X[right_indices], y[right_indices])\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.split_feature is None:\n",
    "            return self.prediction\n",
    "        else:\n",
    "            predictions = np.zeros(len(X))\n",
    "            left_indices = X[:, self.split_feature] <= self.split_value\n",
    "            right_indices = X[:, self.split_feature] > self.split_value\n",
    "\n",
    "            predictions[left_indices] = self.left.predict(X[left_indices])\n",
    "            predictions[right_indices] = self.right.predict(X[right_indices])\n",
    "\n",
    "            return predictions\n",
    "\n",
    "\n",
    "# Define the Gradient Boosting Regressor\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.full(len(y), np.mean(y))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residual = y - y_pred\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X, residual)\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for tree in self.estimators:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# Initialize and train the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f37fbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "046e2da1",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5acc976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class GradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.full(len(y), np.mean(y))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residual = y - y_pred\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X, residual)\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for tree in self.estimators:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_depth': self.max_depth\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4612602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 100}\n",
      "Mean Squared Error: 52.373232214161746\n",
      "R-squared: 0.9645012146661712\n"
     ]
    }
   ],
   "source": [
    "# Your dataset split into X_train, X_test, y_train, y_test\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to find the best parameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "# Evaluate the model with the best parameters\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e1628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f325381f",
   "metadata": {},
   "source": [
    "\n",
    "### Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "**A weak learner** in the context of Gradient Boosting is a model that's only slightly better than random guessing on a given problem. It's a base model that performs just a bit better than chance. In Gradient Boosting, decision trees are commonly used as weak learners. These trees are often shallow (low depth) to prevent overfitting and make them weak enough to focus on the residual errors of the previous models.\n",
    "\n",
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "**The intuition** behind Gradient Boosting involves creating a strong model by iteratively improving upon the errors of previous models. It works in a stepwise manner, where each subsequent model corrects the errors made by the previous ones. By combining weak learners sequentially, it constructs a powerful ensemble that can fit complex patterns in the data.\n",
    "\n",
    "### Q6. How does the Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "**Gradient Boosting** builds an ensemble by sequentially fitting new models to the residual errors made by the previous models. Here's a simplified outline of the process:\n",
    "\n",
    "1. **Initialize with a simple model:** The algorithm starts with a simple model that predicts the mean (or another constant) for the target variable.\n",
    "2. **Fit a weak learner (typically a decision tree):** It then fits a weak learner to the residuals (errors) of the initial model. This weak learner focuses on correcting the errors made by the initial model.\n",
    "3. **Iteratively build subsequent models:** It continues this process, where each new model focuses on the errors that the ensemble of previous models has made, gradually reducing the overall error.\n",
    "\n",
    "### Q7. What are the steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm?\n",
    "\n",
    "**Mathematically**, the Gradient Boosting algorithm can be understood in steps:\n",
    "\n",
    "1. **Initialize with a constant:** Start by predicting the mean (or another simple value) for the target variable.\n",
    "2. **Compute residuals:** Calculate the residuals (errors) by subtracting the predicted values from the actual target values.\n",
    "3. **Fit a weak learner to residuals:** Train a weak learner (usually a decision tree) on these residuals to predict these errors.\n",
    "4. **Update predictions:** Combine the predictions of the weak learner with the previous predictions. This update is performed with a learning rate, controlling the contribution of each model to the ensemble.\n",
    "5. **Repeat steps 2-4:** Iterate this process, focusing each new model on the errors that the ensemble of previous models has made, thereby reducing the overall error.\n",
    "\n",
    "These steps, when performed iteratively, gradually improve the overall model by minimizing the residuals and learning from the mistakes made by the previous models.\n",
    "\n",
    "Understanding these steps mathematically helps in grasping how each subsequent model is fitted to the errors of the previous model, gradually improving the predictive capability of the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e927cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67459fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3e7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
